{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example was based on the lime example https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data, training a classifier\n",
    "For this tutorial, we'll be using the 20 newsgroups dataset. In particular, for simplicity, we'll use a 2-class subset: atheism and christianity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "class_names = ['atheism', 'christian']\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "model.fit(train_vectors, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9209302325581395\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_vectors)\n",
    "print('F1 score:', sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from m_lime.explanations.linear import ExplainLinear\n",
    "from m_lime.explanations.visualization import ImagePlot\n",
    "from m_lime.densities.density_vae import DensityVAE\n",
    "from m_lime.densities.density_kde import DensityKDE\n",
    "from m_lime.densities.density_kde_pca import DensityKDEKPCA, DensityKDEPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id: 83\n",
      "Probability(christian) = [[0.534 0.466]]\n",
      "True class: atheism\n",
      "True class number: 0\n",
      "\n",
      "Text:\n",
      "From: johnchad@triton.unm.edu (jchadwic)\n",
      "Subject: Another request for Darwin Fish\n",
      "Organization: University of New Mexico, Albuquerque\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: triton.unm.edu\n",
      "\n",
      "Hello Gang,\n",
      "\n",
      "There have been some notes recently asking where to obtain the DARWIN fish.\n",
      "This is the same question I have and I have not seen an answer on the\n",
      "net. If anyone has a contact please post on the net or email me.\n",
      "\n",
      "Thanks,\n",
      "\n",
      "john chadwick\n",
      "johnchad@triton.unm.edu\n",
      "or\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'float32' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-baf08638d03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Text:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsgroups_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx_explain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdocument_explain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewsgroups_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'float32' is not defined"
     ]
    }
   ],
   "source": [
    "idx = 83\n",
    "# newsgroups_test.data[idx]\n",
    "print('Document id: %d' % idx)\n",
    "# print(model.predict(test_vectors))\n",
    "print('Probability(christian) =', model.predict_proba(test_vectors[idx]))\n",
    "\n",
    "print('True class: %s' % class_names[newsgroups_test.target[idx]])\n",
    "print('True class number:',newsgroups_test.target[idx] )\n",
    "print()\n",
    "print('Text:')\n",
    "print(newsgroups_test.data[idx])\n",
    "x_explain = test_vectors[idx].reshape(1, -1).toarray().astype(float32)\n",
    "document_explain = newsgroups_test.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating an Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_ = torch.from_numpy(train_vectors.toarray().astype(np.float32), )\n",
    "labels = torch.from_numpy(newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1079, 23035])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1079, 23035])\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors_.size()) #, newsgroups_train.target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(train_vectors_, labels)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = test_vectors[idx].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = DensityVAE(input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1582013226511/work/torch/csrc/utils/python_arg_parser.cpp:698: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, Number alpha)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<m_lime.densities.density_vae.DensityVAE at 0x147ee7b2e950>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density.fit(train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation for  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Double but got scalar type Float for argument #2 'mat2' in call to _th_mm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-ea3764fc438e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mexplain_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExplainLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdensity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SGD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m result = explain_linear.explain_instance(\n\u001b[0;32m---> 10\u001b[0;31m     x_explain=x_explain, r=0.1, class_index=class_to_explain, tol=0.1)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'index: {:} - prediction: {:}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_explain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/density-lime/src/m_lime/explanations/linear.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, x_explain, r, class_index, epochs, n_samples, tol, epochs_max)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             )\n\u001b[1;32m     84\u001b[0m             \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/density-lime/src/m_lime/densities/density_vae.py\u001b[0m in \u001b[0;36msample_radius\u001b[0;34m(self, x_exp, r, n_samples, random_state)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mx_exp_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mmu_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_exp_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmu_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/density-lime/src/m_lime/densities/density_vae.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_encoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_log_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/density-lime3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/density-lime3.7/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/density-lime3.7/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2' in call to _th_mm"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "class_to_explain = 0\n",
    "print('Explanation for ', class_to_explain)\n",
    "try:\n",
    "    del explain_linear\n",
    "except:\n",
    "    pass\n",
    "explain_linear = ExplainLinear(model_predict=model, density=density, linear_model='SGD', verbose=True)\n",
    "result = explain_linear.explain_instance(\n",
    "    x_explain=x_explain, r=0.1, class_index=class_to_explain, tol=0.1)\n",
    "y = 'index: {:} - prediction: {:}'.format(index, np.argmax(y_explain))\n",
    "fig, axis = plt.subplots(1, figsize=(4.5, 4.5))\n",
    "ax = plot_instances(axis, x_explain, y)\n",
    "a = ImagePlot.plot_importance(result['importance'], standardization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_exp = exp_NLS.get_text_explanation(x_explain, document=document_explain, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_betas(dict_exp):\n",
    "    fig, ax = plt.subplots()\n",
    "    width = 0.35 \n",
    "    ind = np.arange(len(dict_exp['words'][::-1]))\n",
    "    bar_pos = plt.barh(ind-width/2, dict_exp['betas_document'][::-1], width, label='Atheism')\n",
    "    plt.title('Atheism Class Feature Importance')\n",
    "    bar_neg = plt.barh(ind + width/2, dict_exp['betas_document_neg'][::-1], width, label='Christian')\n",
    "    plt.title('Atheism Christian Class Feature Importance')\n",
    "    ax.set_yticks(ind)\n",
    "    ax.set_yticklabels(dict_exp['words'][::-1])\n",
    "    ax.grid(True, axis='y')\n",
    "    leg = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_betas(dict_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_exp = exp_NLS.explain_graphical(x_explain, document=document_explain, num_features=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_features[dict_exp['indices_words']], dict_exp['indices_words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_html = exp_NLS.document_html(x_explain, document=document_explain, num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = dict_exp['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "prohibitedWords = ['on', 'Random', 'Words']\n",
    "big_regex = re.compile('|'.join(map(re.escape, prohibitedWords)))\n",
    "# the_message = big_regex.sub(\"<replaced>\", 'this message contains Some really Random Words')\n",
    "text = re.sub(r\"\\b(%s)\\b\" % \"|\".join(words), replace, text)\n",
    "the_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(matched):\n",
    "    # Matched.group(0) is the word that was found\n",
    "    # Return the replacement\n",
    "    return \"REPLACEMENT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['a', 'an']\n",
    "text = 'A word here, an b a'\n",
    "text = re.sub(r\"\\b(%s)\\b\" % \"|\".join(words), replace, text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_html(x_explain, document, num_features=10, tokenizer=None):\n",
    "    exp = exp_NLS.get_text_explanation(x_explain, document, num_features=num_features)\n",
    "    if tokenizer is None:\n",
    "        return None\n",
    "    document_html = document\n",
    "    document_tokens = tokenizer(document)\n",
    "    \n",
    "    format_i = '<b style=\"background-color:Tomato;\">' \n",
    "    format_e = '</b>'\n",
    "    for words in exp['words']:\n",
    "        if words in document_tokens:\n",
    "            document_html = re.sub(r\"\\b(%s)\\b\" % \"|\".join(words), '{:}{:}{:}'.format(\n",
    "                format_i, words, format_e), document_html)\n",
    "#             document_html = re.sub(words , '{:}{:}{:}'.format(format_i, words, format_e) , document_html)\n",
    "#     for words in document_tokens:\n",
    "#         if words in exp['words']:\n",
    "#             document_html += '<b>{:}</b>'.format(words)\n",
    "#         else:\n",
    "#             document_html += words\n",
    "#         print(words)\n",
    "        \n",
    "    print('************************')\n",
    "    print(document_html)\n",
    "    return TextHTML(document_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextHTML:\n",
    "    def __init__(self, html):\n",
    "        self.html = html\n",
    "    def _repr_html_(self):\n",
    "        return self.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _segment_with_tokens(text, tokens):\n",
    "    \"\"\"Segment a string around the tokens created by a passed-in tokenizer\"\"\"\n",
    "    list_form = []\n",
    "    text_ptr = 0\n",
    "    for token in tokens:\n",
    "        inter_token_string = []\n",
    "        while not text[text_ptr:].startswith(token):\n",
    "            inter_token_string.append(text[text_ptr])\n",
    "            text_ptr += 1\n",
    "            if text_ptr >= len(text):\n",
    "                raise ValueError(\"Tokenization produced tokens that do not belong in string!\")\n",
    "        text_ptr += len(token)\n",
    "        if inter_token_string:\n",
    "            list_form.append(''.join(inter_token_string))\n",
    "        list_form.append(token)\n",
    "    if text_ptr < len(text):\n",
    "        list_form.append(text[text_ptr:])\n",
    "    return list_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = document_html(x_explain, document, num_features=10, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = SparseMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1 = make_pipeline(vectorizer, sparse_matrix, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names, random_state=65464)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(newsgroups_test.data[idx], c_1.predict_proba, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_exp = exp_NLS.explain_graphical(x_explain, document=document_explain, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_atheism = newsgroups_train.target[indices_atheism]\n",
    "x_train = np.array(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_atheism = np.where(newsgroups_train.target == 0)[0]\n",
    "x_train_atheism = x_train[indices_atheism]\n",
    "important_words = x_train_atheism[:, dict_exp['indices_all_words']]\n",
    "sum_atheism = np.sum(important_words, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_christian = np.where(newsgroups_train.target == 1)[0]\n",
    "x_train_christian = x_train[indices_christian]\n",
    "important_words_christian = x_train_christian[:, dict_exp['indices_all_words']]\n",
    "sum_christian = np.sum(important_words_christian, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10), dpi=200)\n",
    "ax.barh(names_features[dict_exp['indices_all_words']], sum_christian, label='christian')\n",
    "ax.barh(names_features[dict_exp['indices_all_words']], sum_atheism, label='atheism', alpha=0.5)\n",
    "leg = plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 15))\n",
    "y_p = np.arange(len(names_features[dict_exp['indices_all_words']]))\n",
    "vals = sum_atheism-sum_christian\n",
    "colors = ['green' if x > 0 else 'red' for x in vals]\n",
    "ax.barh(y_p, vals, color=colors)\n",
    "ax.set_yticks(y_p)\n",
    "ax.set_yticklabels(names_features[dict_exp['indices_all_words']])\n",
    "x_lim = ax.get_xlim()\n",
    "colors = ['tab:orange', 'tab:blue']\n",
    "for i, y_p_i in enumerate(y_p):\n",
    "    i_c = i%2\n",
    "    ax.plot(x_lim, [y_p_i]*2, c=colors[i_c])\n",
    "ax.set_xlim(x_lim)\n",
    "title = ax.set_title('Atheism - Christain (frequency words)')\n",
    "# leg = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing: headers, footers, quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train_clean = fetch_20newsgroups(subset='train'\n",
    "                                      , remove=('headers', 'footers', 'quotes')\n",
    "                                      , categories=categories)\n",
    "newsgroups_test_clean = fetch_20newsgroups(subset='test'\n",
    "                                     , remove=('headers', 'footers', 'quotes')\n",
    "                                     , categories=categories)\n",
    "class_names = ['atheism', 'christian']\n",
    "vectorizer_clean = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=True)  # ngram_range=(1, 2)\n",
    "train_vectors_clean = vectorizer_clean.fit_transform(newsgroups_train_clean.data)\n",
    "test_vectors_clean = vectorizer_clean.transform(newsgroups_test_clean.data)\n",
    "train_vectors_clean = train_vectors_clean.toarray()\n",
    "test_vectors_clean = test_vectors_clean.toarray()\n",
    "tokenizer_clean = vectorizer_clean.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_features_clean = np.array(vectorizer_clean.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer_clean.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_parameters = [{\n",
    "        'es_give_up_after_nepochs': 20\n",
    "        , 'hidden_size': 100\n",
    "        , 'num_layers': 2\n",
    "        , 'n_classification_labels': 2,\n",
    "    }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in comb_parameters:\n",
    "    model_clean = NLS(\n",
    "        verbose=0\n",
    "        , es=True\n",
    "        , gpu=True\n",
    "        , scale_data=False\n",
    "        , varying_theta0=False\n",
    "        , fixed_theta0=True\n",
    "        , dataloader_workers=0\n",
    "        # , with_mean=False\n",
    "        , **parameter\n",
    "    ) \n",
    "    model_clean.fit(x_train=train_vectors_clean, y_train=newsgroups_train_clean.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model_clean.predict(test_vectors_clean)\n",
    "pred_poba = model_clean.predict_proba(test_vectors_clean)\n",
    "print('Score:', sklearn.metrics.f1_score(newsgroups_test_clean.target, pred, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating an Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alterando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_NLS_clean = ExplainText(model_clean\n",
    "                             , class_names=['atheism', 'christian']\n",
    "                             , names_features=names_features_clean\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_exp_clean = exp_NLS_clean.get_text_explanation(x_explain_clean\n",
    "                                              , document=document_explain_clean\n",
    "                                              , num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = model_clean.get_thetas(x_pred=x_explain_clean, net_scale=True)[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_explain_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_from_text_indices = np.argwhere(x_explain_clean[0] > 0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_doc = names_features_clean[words_from_text_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_r = betas[words_from_text_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_sum = np.abs(betas_r[:, 0] - betas_r[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(betas_sum)[::-1]\n",
    "# names_doc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_betas(dict_exp_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_exp = exp_NLS_clean.explain_graphical(x_explain_clean\n",
    "                                           , document=document_explain_clean\n",
    "                                           , num_features=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_features_clean[dict_exp_clean['indices_words']], dict_exp_clean['indices_words']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = SparseMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_clean = make_pipeline(vectorizer_clean, sparse_matrix, model_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer_clean = LimeTextExplainer(class_names=class_names, random_state=65464)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_clean = explainer_clean.explain_instance(newsgroups_test_clean.data[idx]\n",
    "                                             , c_1.predict_proba\n",
    "                                             , num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_clean.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_clean.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(train_vectors_clean, newsgroups_train_clean.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(test_vectors_clean)\n",
    "sklearn.metrics.f1_score(newsgroups_test_clean.target, pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict(x_explain_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in comb_parameters:\n",
    "    model_1 = NNPredict(\n",
    "        verbose=1\n",
    "        , es=True\n",
    "        , gpu=True\n",
    "        , dataloader_workers=0\n",
    "        , **parameter\n",
    "    )\n",
    "    model_1.fit(x_train=train_vectors, y_train=newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_1.predict(test_vectors)\n",
    "print('Score:', sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = make_pipeline(vectorizer, sparse_matrix, model_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.predict_proba([newsgroups_test.data[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def label_bar(rects, ax, labels=None, offset_y=0.4):\n",
    "    colors = ['blue', 'orange']\n",
    "    N = len(rects)\n",
    "#     for rect, color in zip(rects, colors):\n",
    "    for i in range(N):\n",
    "        rect = rects[i]\n",
    "#         color = colors[i]\n",
    "        width = rect.get_width()\n",
    "        text_width = '{:3.2f}'.format(width)\n",
    "        if labels is None:\n",
    "            text = text_width\n",
    "            ax.annotate(text,\n",
    "                    xy=(rect.get_width() / 2, rect.get_y() - offset_y + rect.get_height() / 2),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\"\n",
    "                    , ha='center'\n",
    "                    , va='bottom',\n",
    "                    size=30)\n",
    "        else:\n",
    "            # alterando\n",
    "            text = labels[i]\n",
    "            ax.annotate(\n",
    "                text_width\n",
    "                , xy=(rect.get_width() / 2, (rect.get_y() + rect.get_height() / 2) - 0.25)\n",
    "                , xytext=(0, -1)  # 3 points vertical offset\n",
    "                , textcoords=\"offset points\"\n",
    "                , ha='center', va='bottom'\n",
    "                , size=13\n",
    "                , color='black'\n",
    "                , horizontalalignment='right'\n",
    "            )\n",
    "            if rect.get_width() > 0:\n",
    "                aling_text = 'right'\n",
    "                off_setx = -3\n",
    "            else:\n",
    "                aling_text = 'left'\n",
    "                off_setx = +3\n",
    "           \n",
    "            ax.annotate(\n",
    "                text\n",
    "                , xy=(rect.get_x(), rect.get_y())\n",
    "                , xytext=(off_setx, 0)  # 3 points vertical offset\n",
    "                , textcoords=\"offset points\"\n",
    "                , ha=aling_text\n",
    "                , va='bottom'\n",
    "                , size=14)\n",
    "#         print(rect.get_x(), rect.get_y(), rect.get_height(), rect.get_width())\n",
    "        \n",
    "     \n",
    "\n",
    "    \n",
    "def simpleaxis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "class SparseMatrix(object):\n",
    "    \"\"\"\n",
    "    Transformation to be used in a sklearn pipeline\n",
    "    check if a array is sparse.\n",
    "    # TODO: The NLS, LLS, NNPredict should accept sparse array\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(x):\n",
    "        if issparse(x):\n",
    "            return x.toarray()\n",
    "        return x\n",
    "\n",
    "\n",
    "class ExplainText(object):\n",
    "    def __init__(self, model, class_names, names_features):\n",
    "        \"\"\"\n",
    "        :param model: NLS model;\n",
    "        :param class_names: class names to be utilized in the plot;\n",
    "        :param names_features: names of the features.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.class_names = np.array(class_names)\n",
    "        self.names_features = np.array(names_features)\n",
    "\n",
    "    def get_text_explanation(self, x_explain, document, num_features=10):\n",
    "        \"\"\"\n",
    "        Get the explanation of text document.\n",
    "        :param x_explain: document to be explained, should be vectorized;\n",
    "        :param document: document in text format;\n",
    "        :param num_features: number of features to produce the explanation.\n",
    "        :return: betas values and words correspondent to the explanation.\n",
    "        \"\"\"\n",
    "        explanation = self.model.get_thetas(x_pred=x_explain, net_scale=True)\n",
    "        betas = explanation[2][0]\n",
    "        words_from_text_indices = np.argwhere(x_explain[0] != 0).reshape(-1)\n",
    "                \n",
    "        # Prediction from the model\n",
    "        prediction = self.model.predict(x_explain).reshape(-1)\n",
    "        predict_proba = self.model.predict_proba(x_explain).reshape(-1)\n",
    "        ind_pred_proba = np.argsort(predict_proba)[::-1]\n",
    "\n",
    "        # Get the col_number of the predict and the second classes.\n",
    "        col_betas = ind_pred_proba[0]\n",
    "        col_betas_neg = ind_pred_proba[1]\n",
    "        \n",
    "        # Get the betas for the predict and second (neg) classes.\n",
    "        betas_document = betas[words_from_text_indices, col_betas]\n",
    "        betas_document_neg = betas[words_from_text_indices, col_betas_neg]\n",
    "\n",
    "        betas_final = betas_document - betas_document_neg\n",
    "        words_features_document = self.names_features[words_from_text_indices].reshape(-1)\n",
    "\n",
    "        # Ranking the betas by absolute value - crescent order.\n",
    "        beta_0_abs = np.abs(betas_final)\n",
    "        betas_rank_ind = np.argsort(beta_0_abs)[::-1]\n",
    "        # Selecting the first num_features important features.\n",
    "        betas_rank_ind = betas_rank_ind[:num_features] \n",
    "        \n",
    "        # Geting the important words.\n",
    "        words_features_document_rank = words_features_document[betas_rank_ind]\n",
    "\n",
    "        return dict(betas=betas_final[betas_rank_ind]\n",
    "                    , betas_document=betas_document[betas_rank_ind]\n",
    "                    , betas_document_neg=betas_document_neg[betas_rank_ind]\n",
    "                    , words=words_features_document_rank\n",
    "                    , prediction=prediction\n",
    "                    , prediction_proba=predict_proba\n",
    "                    , ind_class_sorted=ind_pred_proba\n",
    "                    , document=document\n",
    "                    , indices_words=words_from_text_indices[betas_rank_ind]\n",
    "                    , indices_all_words=words_from_text_indices\n",
    "                    )\n",
    "\n",
    "    def document_html(self, x_explain, document, num_features=10, tokenizer=None):\n",
    "        exp = self.get_text_explanation(x_explain, document, num_features=num_features)\n",
    "        if tokenizer is None:\n",
    "            return None\n",
    "        document_html = ''\n",
    "        document_tokens = tokenizer(document)\n",
    "        for words in document_tokens:\n",
    "            if words in exp['words']:\n",
    "                document_html += words\n",
    "            print(words)\n",
    "\n",
    "\n",
    "    def explain_graphical(self, x_explain, document, num_features=10):\n",
    "        exp = self.get_text_explanation(x_explain, document, num_features=num_features)\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 5), dpi=200)\n",
    "        colors = ['green', 'red']\n",
    "        rects1 = axs[0].barh(self.class_names[::-1], exp['prediction_proba'][::-1], color=colors[::-1])\n",
    "        axs[0].set_ylim([-3, 2])\n",
    "        simpleaxis(axs[0])\n",
    "        axs[0].set_xticks([])\n",
    "        axs[0].tick_params('y', labelsize=20)\n",
    "#         for rect, color in zip(rects1, colors):\n",
    "#             rect.set_color(color)\n",
    "        axs[0].set_title('Predicted Probabilities', size=20)\n",
    "        \n",
    "        label_bar(rects1, axs[0])\n",
    "        names = exp['words'][::-1]\n",
    "        vals = exp['betas'][::-1]\n",
    "        class_names_sorted = self.class_names[exp['ind_class_sorted']]\n",
    "        self.get_plot_feature_importance(axs[1], names, vals, class_names_sorted)\n",
    "        \n",
    "        \n",
    "        simpleaxis(axs[2])\n",
    "        axs[2].set_xticks([])\n",
    "        axs[2].set_yticks([])\n",
    "        axs[2].text(0, 1, '\\n' + exp['document'], style='italic', wrap=True, va='top')\n",
    "        axs[2].set_title('Document to Explain', size=20)\n",
    "        plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\n",
    "        return fig, axs\n",
    "\n",
    "    def get_plot_feature_importance(self, ax, names, vals, class_names):\n",
    "        colors = ['green' if x > 0 else 'red' for x in vals]\n",
    "        pos = np.arange(len(vals))\n",
    "        rects2 = ax.barh(pos, vals, align='center', color=colors)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        label_bar(rects2, ax, labels=names)\n",
    "        ax.axvline(0, color='black', lw=2)\n",
    "        ax.set_title('Features Importance', size=20)\n",
    "        y_lim = np.array(ax.get_ylim())\n",
    "        ax.set_ylim(y_lim+np.array([0, 1.6]))\n",
    "        \n",
    "        ax.annotate('  ' + class_names[0], xy=(0,y_lim[1]), size=22, color='green', ha='left')\n",
    "        ax.annotate(class_names[1] + '  ', xy=(0,y_lim[1]), size=22, color='red', ha='right')\n",
    "        simpleaxis(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_NLS = ExplainText(model, class_names=['atheism', 'christian'], names_features=names_features)\n",
    "# dict_exp = exp.get_text_explanation(x_explain, document=document_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_exp = exp_NLS.get_text_explanation(x_explain, document=document_explain, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_exp = exp_NLS.explain_graphical(x_explain, document=document_explain, num_features=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
